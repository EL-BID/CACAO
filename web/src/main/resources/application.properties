# This application name. Also used as the 'service ID' for 'auto discovery' at 'Eureka'.
spring.application.name=CACAO

# Application identity
app.name=CACAO
app.full.name=System for Consulting and Storing Accounting Data and Organizational Support
app.start=Start CACAO
app.description=CACAO is a system maintained by the Tax Administration for receiving and maintaining files with accounting data of taxpayers.

# Tax Administration identification
ta.short.name=<Tax Admin>
ta.full.name=<Tax Admin Full Name>
ministry.short.name=<MF>
ministry.full.name=<Ministry of Finance>
ta.full.country=<Country Name>
ta.full.address= <T.A. Full Address>
# Qualifiers
taxpayer.qualifier.1=Economic Sector
taxpayer.qualifier.2=Legal Entity
taxpayer.qualifier.3=County
taxpayer.qualifier.4=Size of Company
taxpayer.qualifier.5=Tax Regime

# Provide the URL for registering itself as a service at Eureka Server
eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/
# Indicates the interval of heartbeats that the client sends to the server
eureka.instance.leaseRenewalIntervalInSeconds=5
# Indicates the time in seconds that the Eureka server waits since it received the last heartbeat from a client before it can remove that client from its registry
eureka.instance.leaseExpirationDurationInSeconds=10

# Active profile
spring.profiles.active=default

# Port number for use with embedded Tomcat (ignored if this application is deployed as WAR file)
server.port=8888

# Icons used in different places of CACAO
# If you want to provide external files as alternatives to these ones, just inform their locations prepended with the fixed location name '/images2/' (the number 2 here denotes an alternative external folder)
# The location of this external folder must be informed using the parameter 'images2.location'
#images2.location=file:/path/to/external/location/with/images
logo.cacao=images/Cacao-logo.png
logo.cacao.small=images/Cacao-logo-small.png
logo.miniature=images/favicon.ico

# CSS configuration for colors presented at the first page
initial.screen.background.color=linear-gradient(-45deg, #7b3f00, #fcd22f, #964b00)

# Hostname of ElasticSearch Service
es.host=127.0.0.1

# Port number of ElasticSearch Service
es.port=9200

# Hostname of Kibana Service
kibana.host=127.0.0.1

# Port number of Kibana Service
kibana.port=5601

# Endpoint of Kibana Service (external address redirected from local proxy)
kibana.endpoint=/kibana

# Local link for Kibana Service (either the location of local proxy or some other external URL)
kibana.menu.link=/kibana

# Default page size for all pages
default.page.size=15

# Default color for tables
table.color.level.1=#FFA500
table.color.level.2=#ffe854
table.color.level.3=#FFFDDE

#Default color for customers vs suppliers difference
table.customers.vs.suppliers.difference=#FFFDDE

# A number of past periods to search in analysis
search.past.periods=5

# Spring cloud stream kafka topics properties
spring.cloud.stream.bindings.receiveProcessedFile-in-0.destination=processed-files
spring.cloud.stream.bindings.receiveProcessedFile-in-0.group=publish
spring.cloud.stream.bindings.fileUploaded-out-0.destination=uploaded-files
spring.cloud.stream.bindings.fileUploaded-out-0.group=validate
spring.cloud.stream.bindings.fileUploaded-out-0.producer.partition-selector-name=UploadedFileSelector
spring.cloud.stream.bindings.fileUploaded-out-0.producer.partition-key-extractor-name=UploadedFilePartitioner
spring.cloud.stream.bindings.fileUploaded-out-0.producer.partition-count=10

spring.cloud.function.definition=receiveProcessedFile

# Minimum number of partitions for every KAFKA topic
# Specific KAFKA topics may have a higher number of partitions using the 'spring.cloud.stream.bindings.<binding-name>.producer.partition-count' property
spring.cloud.stream.kafka.binder.min-partition-count=10

# The number of threads that will be used for each KAFKA consumer group
# Should be less or equal to the number of partitions per topic
spring.cloud.stream.default.consumer.concurrency=4

# In the case the KAFKA topic already exists, let the application increase the number of partitions if necessary
spring.cloud.stream.kafka.binder.autoAddPartitions=true


# Location of KAFKA Brokers (may be overridden in production environment, see 'KAFKA_BROKERS' at Dockerfile and setup.sh)
spring.cloud.stream.kafka.binder.brokers=localhost:9092

# KAFKA Max poll interval in milliseconds (default: 300000)
spring.kafka.consumer.properties.max.poll.interval.ms=900000

# KAFKA Request timeout in milliseconds (default: 40000)
spring.kafka.consumer.properties.request.timeout.ms=300000

# KAFKA Heartbeat interval in milliseconds (default: 3000)
spring.kafka.consumer.properties.heartbeat.interval.ms=1000

# KAFKA Session timeout in milliseconds (default: 10000)
spring.kafka.consumer.properties.session.timeout.ms=600000

# KAFKA Maximum poll records (default: 500)
spring.kafka.consumer.properties.max.poll.records=100

# Set this to TRUE in order to use an embedded KAFKA (should only be used at development environment)
#use.kafka.embedded=false

# Configuration for running this application behind a reverse proxy
server.forward-headers-strategy=native
server.tomcat.remote-ip-header=x-forwarded-for
server.tomcat.protocol-header=x-forwarded-proto

# Configuration for making use of SSL over HTTP 
# Unless the application is sitting behind a reverse proxy terminating SSL connections (e.g. NGINX),
# you should configure these parameters for securing the web application
#server.ssl.key-store-type=PKCS12
#server.ssl.key-store=file:${user.home}/.cacao/cert.p12
#server.ssl.key-store-password=123456
#server.ssl.key-alias=cacao
#server.ssl.enabled=true

# Configuration for making use of SSL for many purposes (e.g. encrypting SMTP password, encrypting API tokens, etc.)
#mail.ssl.key-store=file:${user.home}/.cacao/cert.p12
#mail.ssl.key-store-password=123456
#mail.ssl.enabled=true

# Set to 'false' if you want to make it possible to update Thymeleaf templates on the fly in development environment
spring.thymeleaf.cache=true

# Set to positive number of minutes if you want SPRING to reload periodically the information stored at message properties
# Set to 0 to keep unchanged (i.e. the application loads message properties only once)
auto.reload.properties=0

spring.servlet.multipart.max-file-size=1GB

spring.servlet.multipart.max-request-size=1GB

# Maximum number of entries it expects to find in incoming ZIP file
max.entries.per.uploaded.zip=100

# Enable response compression
server.compression.enabled=true

# Coma-separated list of mime types that should be compressed
server.compression.mime-types=text/html,text/xml,text/plain,text/css,text/javascript,application/javascript,application/json

# Compress the response only if the response size is at least 1 KB
server.compression.min-response-size=1024

# Enable HTTP/2 support, if the current environment supports it
server.http2.enabled=true

# Minimum number of documents stored for auto creation of corresponding index pattern at Kibana space. Set to zero if
# you want to disable this auto-creation feature
docs.min.auto.index.pattern=1

# maximum amount of time the server will wait for the client to make their request after connecting before the connection is closed
server.connection-timeout=300000

# Connection timeout for use of ElasticSearch REST API
spring.elasticsearch.rest.connection-timeout=5m
spring.elasticsearch.connection-timeout=5m
spring.elasticsearch.socket-timeout=5m

# First master user when database is empty
first.master.user.login=admin@admin
first.master.user.password=123456
first.master.user.name=Administrator

# Default system language
cacao.user.language=en

# Default system country
cacao.user.country=US

# auto.create.users=true for creating new users automatically based on e-mail and fullname provided by OIDC providers
auto.create.users=true

# enable periodic resource monitor metrics collector
#resource.monitor=true

# documents storage
storage.incoming.files.original.dir=/var/cacao/incoming_files/original

# temporary storage for creating PARQUET files (e.g. for SYNC operation)
storage.parquet.files.temporary.dir=/var/cacao/parquet/temporary

# tells if the application should overwrite any existing built-in domain table at startup
built-in.domain.tables.overwrite=false

# Enable at startup checking and fixing all ElasticSearch indices mappings that are not compatible with current model objects
compatibilize.indices.at.start=true

# The 'presentation.mode' is a boolean property for determining if this application should run for the purpose of 'PRESENTING'
# the system itself. If it's set to true, some features will not be available (e.g.: it won't be possible to upload files). If
# it's set to false, this application will run normally (i.e.: no restrictions)
presentation.mode=false

# Enable/disable the SYNC publisher service. It's disabled by default. If you enable it, other CACAO servers
# will be able to request updates from this server. Almost all information will be available through RESTful
# '/api/sync' endpoint, as long as the requesting user has SYSADMIN profile. 
#sync.publisher.enabled=true

# If SYNC publisher service is enabled, you may impose an additional filter over IP addresses that are permitted
# to invoke the RESTful SYNC API. Any other IP address will be rejected, despite of token API validity.
# This filter is applied in addition to the other rules (e.g. the request must also provide a valid API token)
# You may optionally use wildcards ('*') for matching range of IP addresses.
# You may optionally separate different filters using pipe ('|')
# In case you don't want to use this filter, either inform a simple wildcard character ('*') or keep it empty.
#sync.publisher.filter.host=*

# If this application is operating as a consumer of 'SYNC' (i.e. this application receives data from another server) this parameter
# indicates the number of concurrent threads for speed up multiple SYNC operations simultaneously
sync.consumer.threads=4

# Set this flag to true if you want to display plenty of messages at LOG file in SYNC operations, when debugging it
#sync.full.debug=false

# Uncomment and set this line to a valid user account (e-mail) at CACAO
# for giving him full access to Kibana. Should only be used for initial
# configuration if the RBAC and SSL were activated at Elastic Stack 
#kibana.superuser=

# Regular expression to be matched with hostnames related to internal trusted services (such as Kibana and Elastic Search)
# It's only used for setting up SSL configuration related to these services
ssl.trust.server=^(kibana|es)\d*

# Profile / Privilege Mappings
# For each privilege identified by its name (SystemPrivilege enum constant), there should be a list of corresponding user profiles identified by their names (UserProfile enum constant).
privilege.ADMIN_OPS=SYSADMIN
privilege.CONFIG_API_TOKEN=SYSADMIN,SUPPORT,DECLARANT
privilege.CONFIG_SYSTEM_MAIL=SYSADMIN
privilege.INTERPERSONAL_READ_ALL=SYSADMIN,SUPPORT,MASTER,AUTHORITY
privilege.INTERPERSONAL_WRITE=SYSADMIN,SUPPORT,AUTHORITY
privilege.SYNC_OPS=SYSADMIN
privilege.TAX_DECLARATION_READ=SYSADMIN,SUPPORT,AUTHORITY,MASTER,DECLARANT
privilege.TAX_DECLARATION_READ_ALL=SYSADMIN,SUPPORT,MASTER,AUTHORITY
privilege.TAX_DECLARATION_WRITE=SYSADMIN,DECLARANT
privilege.TAX_DECLARATION_WRITE_EMPTY=SYSADMIN,SUPPORT,MASTER
privilege.TAX_TEMPLATE_WRITE=SYSADMIN,SUPPORT
privilege.TAX_DOMAIN_TABLE_WRITE=SYSADMIN,SUPPORT
privilege.TAXPAYER_READ=SYSADMIN,SUPPORT,AUTHORITY,MASTER,DECLARANT
privilege.TAXPAYER_READ_ALL=SYSADMIN,SUPPORT,AUTHORITY,MASTER
privilege.TAXPAYER_WRITE=SYSADMIN,SUPPORT
privilege.USER_RECENT_READ=SYSADMIN
privilege.USER_HISTORY_READ=SYSADMIN,SUPPORT
privilege.USER_READ=SYSADMIN,SUPPORT
privilege.USER_WRITE=SYSADMIN,SUPPORT
privilege.TAX_REPORT_READ=SYSADMIN,SUPPORT,AUTHORITY,MASTER